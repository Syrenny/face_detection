{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f05b5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "#import av\n",
    "import cv2\n",
    "import onnxruntime\n",
    "import albumentations as A\n",
    "from onemetric.cv.utils.iou import box_iou_batch\n",
    "\n",
    "import tracker\n",
    "from tracker.byte_tracker import STrack\n",
    "\n",
    "\n",
    "class Yolov5Detector():\n",
    "    confidence_thres = 0.5\n",
    "    iou_thres = 0.5\n",
    "    min_score = 0.5\n",
    "    \n",
    "    byte_tracker_args = {\n",
    "        \"track_thresh\": 0.25\n",
    "        \"track_buffer\": 30\n",
    "        \"match_thresh\": 0.8\n",
    "        \"aspect_ratio_thresh\": 3.0\n",
    "        \"min_box_area\":= 1.0\n",
    "        \"mot20\": False\n",
    "        \"MEAN\": 0\n",
    "        \"STD\": 0.5\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_path='best.onnx', size=(1280, 1280), classes={0: 'face'}) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MyDetector object.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the model file.\n",
    "            size (tuple): Size of the input image.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self._session = onnxruntime.InferenceSession(model_path)\n",
    "        self.input_name = self._session.get_inputs()[0].name\n",
    "        self.output_names = [self._session.get_outputs()[0].name]\n",
    "        self._size = size\n",
    "        self.classes = classes\n",
    "        \n",
    "    def _preprocess(self, rgb_img):\n",
    "        \"\"\"\n",
    "        Performs preprocessing transformation for model.\n",
    "\n",
    "        Args:\n",
    "            rgb_img (numpy.ndarray): Input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the transformation.\n",
    "        \"\"\"\n",
    "        mean=(0.485, 0.456, 0.406)\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "        transform = A.Compose([\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            A.LongestMaxSize(max_size=self._size[0]),\n",
    "            A.PadIfNeeded(min_height=self._size[0], min_width=self._size[0], border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)),\n",
    "        ])\n",
    "\n",
    "        # Применение аугментации к изображению\n",
    "        return transform(image=rgb_img)['image']\n",
    "        \n",
    "    \n",
    "    def forward(self, rgb_img):\n",
    "        \"\"\"\n",
    "        Performs forward pass on the input image.\n",
    "\n",
    "        Args:\n",
    "            rgb_img (numpy.ndarray): Preprocessed input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the forward pass.\n",
    "        \"\"\"\n",
    "        input_image = np.expand_dims(rgb_img, axis=0).astype('float32')\n",
    "        input_image = np.transpose(input_image, [0, 3, 1, 2])\n",
    "        return input_image, self._session.run(None, {self.input_name: input_image})[0][0]\n",
    "    \n",
    "    def post_process(self, output):\n",
    "        \"\"\"\n",
    "        Post-process the output of the model.\n",
    "\n",
    "        Args:\n",
    "            output (tuple): Output of the model.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Processed bounding boxes and class IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def draw_detections(self, img, box, score, class_id=0, tracker_id=None):\n",
    "        \"\"\"\n",
    "        Draws bounding boxes and labels on the input image based on the detected objects.\n",
    "\n",
    "        Args:\n",
    "            img: The input image to draw detections on.\n",
    "            box: Detected bounding box.\n",
    "            score: Corresponding detection score.\n",
    "            class_id: Class ID for the detected object.\n",
    "            tracker_id: Tracking ID of bbox \n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the coordinates of the bounding box\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Create the label text with class name and score\n",
    "        keep = True\n",
    "        try: \n",
    "            if tracker_id is not None:\n",
    "                label = f\"face: {score:.2f}\"\n",
    "            else: \n",
    "                label = f\"face: {score:.2f} track_id: {tracker_id}\"\n",
    "        except KeyError:\n",
    "            keep = False\n",
    "        if keep:\n",
    "            # Retrieve the color for the class ID\n",
    "            color_palette = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "            color = color_palette[class_id]\n",
    "\n",
    "            # Draw the bounding box on the image\n",
    "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "            # Calculate the dimensions of the label text\n",
    "            (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "\n",
    "            # Calculate the position of the label text\n",
    "            label_x = x1\n",
    "            label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10\n",
    "\n",
    "            # Draw a filled rectangle as the background for the label text\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                (int(label_x), int(label_y - label_height)),\n",
    "                (int(label_x + label_width), int(label_y + label_height)),\n",
    "                color,\n",
    "                cv2.FILLED,\n",
    "            )\n",
    "\n",
    "            # Draw the label text on the image\n",
    "            cv2.putText(img, label, (int(label_x), int(label_y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "    def draw_box(self, img):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes on the input image.\n",
    "\n",
    "        Args:\n",
    "            img (numpy.ndarray): Input image.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Image with bounding boxes drawn.\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _match_detections_with_tracks(detections, tracks: List[tracker.byte_tracker.STrack]):\n",
    "        \"\"\"\n",
    "        Matches bboxes with predictions\n",
    "\n",
    "        Args:\n",
    "            detections (dict): Input dictionary with keys \"bboxes\" (xyxy) and \"confidence\".\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Tracker indexes for every bbox, may contain None objects if bbox is not being tracked.\n",
    "        \"\"\"\n",
    "        if not np.any(detections.xyxy) or len(tracks) == 0:\n",
    "            return np.empty((0,))\n",
    "\n",
    "        # converts List[STrack] into format that can be consumed by self._match_detections_with_tracks function\n",
    "        tracks_boxes = np.array([track.tlbr for track in tracks], dtype=float)\n",
    "        iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
    "        track2detection = np.argmax(iou, axis=1)\n",
    "\n",
    "        tracker_ids = [None] * len(detections)\n",
    "\n",
    "        for tracker_index, detection_index in enumerate(track2detection):\n",
    "            if iou[tracker_index, detection_index] != 0:\n",
    "                tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
    "\n",
    "        return np.array(tracker_ids)\n",
    "\n",
    "\n",
    "    def video_run(self, cap):\n",
    "        \"\"\"\n",
    "        Process video frames and draw bboxes.\n",
    "\n",
    "        Args:\n",
    "            cap: Video capture object.\n",
    "\n",
    "        Returns:\n",
    "            io.BytesIO: In-memory file containing the processed video.\n",
    "        \"\"\"\n",
    "        # Create BYTETracker instance\n",
    "        byte_tracker = tracker.byte_tracker.BYTETracker(self.byte_tracker_args)\n",
    "        \n",
    "        # Video stream handling \n",
    "        output_memory_file = io.BytesIO()\n",
    "        output_f = av.open(output_memory_file, 'w', format=\"mp4\")  # Open \"in memory file\" as MP4 video output\n",
    "        stream = output_f.add_stream('h264', str(fps))  # Add H.264 video stream to the MP4 container, with framerate = fps.\n",
    "        ret = True\n",
    "        # Video capturing\n",
    "        while ret:\n",
    "            ret, frame = cap.read()\n",
    "            preprocessed_image, results = self.forward(frame)\n",
    "            detections = {\n",
    "                \"xyxy\": results[:4],\n",
    "                \"confidence\": results[4]\n",
    "            }\n",
    "            \n",
    "            # Tracking detections\n",
    "            tracks = byte_tracker.update(\n",
    "                output_results=np.hstack((detections.xyxy, detections.confidence[:, np.newaxis])),\n",
    "                img_info=frame.shape,\n",
    "                img_size=frame.shape\n",
    "            )\n",
    "            detections.tracker_id = self._match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "            \n",
    "            # Filtering out detections without trackers\n",
    "            mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "            detections.filter(mask=mask, inplace=True)\n",
    "            \n",
    "            # Drawing bboxes with labels\n",
    "            res_img = np.copy(frame)\n",
    "            for xyxy, score, tracker_id in zip(detections.xyxy, detections.confidence, detections.tracker_id):\n",
    "                self.draw_detections(img=res_img, box=xyxy, score=score, tracker_id=tracker_id)\n",
    "            \n",
    "            # Convert image from NumPy Array to frame.\n",
    "            res_img = av.VideoFrame.from_ndarray(res_img, format='bgr24') \n",
    "            packet = stream.encode(frame)  # Encode video frame\n",
    "            output_f.mux(packet)  # \"Mux\" the encoded frame (add the encoded frame to MP4 file).\n",
    "            \n",
    "        # Flush the encoder\n",
    "        packet = stream.encode(None)\n",
    "        output_f.mux(packet)\n",
    "        output_f.close()\n",
    "        return output_memory_file\n",
    "\n",
    "    \n",
    "    def __call__(self, rgb_img):\n",
    "        \"\"\"\n",
    "        Call the MyDetector object to process an input image.\n",
    "\n",
    "        Args:\n",
    "            rgb_img (numpy.ndarray): Input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Processed bounding boxes and class IDs.\n",
    "        \"\"\"\n",
    "        return self.draw_box(rgb_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5698a93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100800, 6)\n",
      "[6.5840483e+00 5.0424957e+00 1.5894703e+01 1.0950976e+01 1.3709068e-06\n",
      " 9.9996531e-01]\n",
      "box [6.5840483e+00 5.0424957e+00 1.5894703e+01 1.0950976e+01 1.3709068e-06\n",
      " 9.9996531e-01]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Выполняем детекцию\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Отображаем результаты с помощью Matplotlib\u001b[39;00m\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(result_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
      "Cell \u001b[0;32mIn[59], line 152\u001b[0m, in \u001b[0;36mYolov5Detector.draw_box\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_box\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Draw bounding boxes on the input image.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        numpy.ndarray: Image with bounding boxes drawn.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     bboxes, class_ids, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    155\u001b[0m     inp_height, inp_width, _ \u001b[38;5;241m=\u001b[39m input_image\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[59], line 78\u001b[0m, in \u001b[0;36mYolov5Detector.post_process\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     76\u001b[0m cx, cy, w, h, confidence, id_class \u001b[38;5;241m=\u001b[39m box\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m, box)\n\u001b[0;32m---> 78\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m \u001b[38;5;241m-\u001b[39m w \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     79\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m h \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     80\u001b[0m boxes\u001b[38;5;241m.\u001b[39mappend([x1, y1, w, h])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Создаем экземпляр детектора\n",
    "detector = Yolov5Detector()\n",
    "\n",
    "# Загружаем изображение для тестирования\n",
    "image_path = 'zidane.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Выполняем детекцию\n",
    "result_image = detector.draw_box(image)\n",
    "\n",
    "# Отображаем результаты с помощью Matplotlib\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f717675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syrenny_kernel",
   "language": "python",
   "name": "syrenny_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
